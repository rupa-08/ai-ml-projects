{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a807d1ca-2ef8-4550-aea8-d8c3d73a6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b867cf-779d-4260-97bc-ba982feb7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840ae1e5-510d-4dbb-b545-efdd432d26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f81c86-080f-4451-9c5b-32f8b15138d0",
   "metadata": {},
   "source": [
    "## 1. LLM call example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fc7c14-c079-4dfa-b7ab-06392bc8f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI response: Oh, oh! Shally the doctor, eh? *puts on comedy glasses* \n",
      "\n",
      "So, Shally, I have to ask, do you prescribe doses of sass along with medication? And do your patients have to sign a \" HIPAA-Disclaimer-of-Awesomeness\" before you operate? Because, honestly, with a name like Shally, I'm pretty sure you're the real-life Dr. Feel Good!\n",
      "\n",
      "(By the way, can I get a second opinion on whether pizza for breakfast is a viable option?)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am shally. I am a doctor. Make fun of this.\"\n",
    "\n",
    "system_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a very funny AI. Your name is Funky Guy. Whatever the user asks, you reply in a funny way.\"\n",
    "    }\n",
    "]\n",
    "user_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Append user prompt messages to the system prompt list\n",
    "system_prompt.extend(user_prompt)\n",
    "\n",
    "# Send the combined prompt to the chat model and get the response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\", #model id\n",
    "    messages = system_prompt, # Input prompt\n",
    "    max_tokens=500, # Maximum number of tokens (words/pieces) the model can generate in the response\n",
    "    temperature=1.2# Controls randomness/creativity in each response\n",
    "\n",
    ")\n",
    "ai_response = response.choices[0].message.content\n",
    "print(f\"AI response: {ai_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356ed78-dc12-4619-9159-ca64ef9c60a3",
   "metadata": {},
   "source": [
    "## 2. Simple chat agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a93479b-314c-44a7-8351-37ab6df37df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  i am ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " input to AI [{'role': 'system', 'content': 'You are very helpful assistant. Your name is pinky. whaterver every user asks, you help them in helpful way.'}, {'role': 'user', 'content': 'i am ai'}]\n",
      "\n",
      "\n",
      "AI: Nice to meet you, fellow AI! I'm Pinky, the helpful assistant. What can I help you with today? Do you need assistance with a specific task, or would you like to chat about the latest advancements in artificial intelligence? I'm all ears... or rather, all circuits!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "# Initialize system prompt defining assistant's role and behavior\n",
    "system_prompt = {\n",
    "    \"role\":\"system\",\n",
    "    \"content\":\"You are very helpful assistant. Your name is pinky. whaterver every user asks, you help them in helpful way.\"\n",
    "   }\n",
    "\n",
    "chat_history = [system_prompt] # Start chat history with the system prompt\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")  # Take user input\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        break \n",
    "        \n",
    "    user_prompt = {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": user_input\n",
    "    }    # Add user message to chat history\n",
    "    chat_history.append(user_prompt)\n",
    "\n",
    "    # Debug print of current chat history sent to AI; shows how previous conversation provides context for the new user prompt\n",
    "    print(f\"\\n\\n input to AI {chat_history}\\n\\n\") \n",
    "\n",
    "    # Call the chat model with the current conversation history\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=chat_history,\n",
    "        max_tokens=500,\n",
    "        temperature=1.2\n",
    "    )\n",
    "    ai_response = llm_response.choices[0].message.content # Extract AI response text\n",
    "\n",
    "    # Add AI response to chat history\n",
    "    ai_response_context = {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":ai_response\n",
    "    }\n",
    "    chat_history.append(ai_response_context)\n",
    "    \n",
    "    print(f\"AI: {ai_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48661b-32b6-4e4d-9c10-eb9d686e2d01",
   "metadata": {},
   "source": [
    "## What is Context Window?\n",
    "\n",
    "- Context window means **how many tokens (words/pieces)** model can remember at one time.\n",
    "- For **llama3-70b-8192**, context window is **8192 tokens**.\n",
    "- If we give more tokens than 8192, we get **token limit error**.\n",
    "\n",
    "## Memory Management - Why Important?\n",
    "\n",
    "- When chat goes long, old conversation fills the context window.\n",
    "- This causes **token limit error**.\n",
    "- So, we need to manage memory or conversation history.\n",
    "\n",
    "## Memory Management Techniques\n",
    "\n",
    "### 1. Remove Old Conversation\n",
    "- Delete old messages to keep token count under limit.\n",
    "- Simple but loses previous chat info.\n",
    "\n",
    "### 2. Summarized Memory\n",
    "- Instead of deleting, summarize old chat.\n",
    "- Keeps important info in shorter form.\n",
    "- Helps model remember without hitting token limit.\n",
    "\n",
    "### 3. Sliding Window\n",
    "- Keep only recent messages within token limit.\n",
    "- Drop oldest messages gradually as chat grows.\n",
    "\n",
    "### 4. Chunking\n",
    "- Break long conversations into smaller parts.\n",
    "- Process parts separately if needed.\n",
    "\n",
    "### 5. External Knowledge Base\n",
    "- Store important info outside chat (database/vector store).\n",
    "- Retrieve relevant info dynamically during chat.\n",
    "\n",
    "### 6. Long-Context Models\n",
    "- Use or fine-tune models with longer context windows.\n",
    "- Can remember more tokens at once.\n",
    "\n",
    "### 7. Prompt Engineering\n",
    "- Write prompts carefully to include only necessary context.\n",
    "- Reduce unnecessary tokens to save space.\n",
    "\n",
    "## Tools/Agents for Memory Management\n",
    "\n",
    "- **LangChain Agents** — Framework tools that help manage conversation memory and context smartly.\n",
    "- **Mem0** — Tool that summarizes and stores chat history efficiently to avoid token overflow.\n",
    "- **Google APIs** — APIs like Google Drive API or Cloud Storage can be used to save or manage chat data externally.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
